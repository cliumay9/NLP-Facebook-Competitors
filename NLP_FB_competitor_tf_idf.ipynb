{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor's Keyword analysis\n",
    "## Done by Calvin Liu\n",
    "## info@kicsv.org & calvinliu@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similar Companies Facebook Keyword analysis\n",
    "#### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries for Keyword Analysis and Term Frequency-Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvinliu/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/calvinliu/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/calvinliu/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/calvinliu/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/calvinliu/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import sklearn as sk \n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CountV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as tfidf\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing nltk library and corpus, we definitely could use other corpus however nltk is easy to use for this example purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can use stanford CoreNLP\n",
    "# Using Stanford CoreNLP\n",
    "# wget http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip\n",
    "# unzip stanford-corenlp-full-2017-06-09.zip\n",
    "# python corenlp.py\n",
    "# from corenlp import *\n",
    "# corenlp = StanfordCoreNLP()  # wait a few minutes...\n",
    "# corenlp.parse(\"Parse this sentence.\")\n",
    "# from nltk.tokenize.stanford import CoreNLPTokenizer\n",
    "\n",
    "import nltk \n",
    "#(Natural Language toolkit)\n",
    "# nltk.download() # Download the nltk corpus\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url, type =\"lxml\"):\n",
    "    \"\"\"\n",
    "    Make our lives easier with urlopen from urllib2 and BeautifulSoup\n",
    "    \"\"\"\n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html,type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(string):\n",
    "    \"\"\"replace html-like characters as empty space\"\"\"\n",
    "    chr = re.compile(\"<.*?>\")\n",
    "    string = re.sub(chr,\"\",string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(section_url):\n",
    "    \"\"\"\n",
    "    Use soup to crawl the webpage and find the user post that \n",
    "    was div and _5pbx user content tags (we can press F12 to \n",
    "    find the pattern and structure)\n",
    "    \"\"\"\n",
    "    # _5pbx userContent _3576, yields us facebook posts that consists of hyperlink\n",
    "    soup =make_soup(section_url)\n",
    "    userContent = soup.find_all(\"div\", \"_5pbx userContent _3576\")\n",
    "    # soup.p can change to soup.find_all(\"div\", \"_5pbx userContent _3576\") something more general use case\n",
    "    posts = [remove_html(str(s)) for s in userContent]\n",
    "    return posts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run this analysis on WeWork, YCombinator, StartX, 500 Startups, InnoSpring, and etc. \n",
    "(We can extend this list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URLs = [\"https://www.facebook.com/WeWork/\", \"https://www.facebook.com/YCombinator/\", \n",
    "             \"https://www.facebook.com/StartX/\", \"https://www.facebook.com/500startups/\",\n",
    "             \"https://www.facebook.com/InnoSpring/\", \"https://www.facebook.com/amplifyla/\",\n",
    "             \"https://www.facebook.com/CcHUBNigeria/\",\"https://www.facebook.com/mattervc/\",\n",
    "             \"https://www.facebook.com/UpWestLabsSV\",\"https://www.facebook.com/parisoma/\",\n",
    "             \"https://www.facebook.com/SandboxSuites/\", \"https://www.facebook.com/angelpadorg/\",\n",
    "             \"https://www.facebook.com/FoundersSpace/\", \"https://www.facebook.com/ZGCSiliconValley/\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts =[]\n",
    "# Obtain the post from all competitors\n",
    "\n",
    "for i in range(len(BASE_URLs)):\n",
    "    # Go to website i, to get user Contents\n",
    "    n = get_posts(BASE_URLs[i])\n",
    "\n",
    "    for post in range(len(n)):\n",
    "        # Add those individual user content as post.\n",
    "        posts.append(n[post])\n",
    "        \n",
    "# Import stop words for english to remove redundant or meaningless words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(u'around')\n",
    "\n",
    "# Remove zgc\n",
    "stop_words.add(u'zgc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of word model\n",
    "\n",
    "def bag(corpus, stop = True, WordOnly = True):\n",
    "    if stop == True:\n",
    "        if WordOnly == True:\n",
    "            vectorizer = CountV(stop_words = stop_words, tokenizer = TreebankWordTokenizer().tokenize)\n",
    "        else:\n",
    "            vectorizer = CountV(stop_words = stop_words)\n",
    "    else:\n",
    "        if WordOnly == True:\n",
    "            vectorizer = CountV(stop_words = None, tokenizer = TreebankWordTokenizer().tokenize)\n",
    "        else:\n",
    "            vectorizer = CountV(stop_words = None)\n",
    "    vect = vectorizer.fit_transform(corpus)\n",
    "    dMat = vect.todense()\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    inverted_vocab = dict([[v,k] for k, v in vocab.items()])\n",
    "    return [vectorizer, dMat, vocab, inverted_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(matrix):\n",
    "    transformer = tfidf(smooth_idf = False)\n",
    "    mat = transformer.fit_transform(matrix)\n",
    "    return mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(l):\n",
    "    j=[]\n",
    "    for word in l:\n",
    "        word= word[1:]\n",
    "        j.append(word)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "import math\n",
    "\n",
    "l = []\n",
    "r = []\n",
    "for i in range(n):\n",
    "    [vectorizer, dMat, vocab, inverted_vocab] = bag(posts, True)\n",
    "    # use term frequency-inverse document frequency\n",
    "    # print(dMat)\n",
    "    tfidf_matrix =frequency(dMat)\n",
    "    most_used_words_tfidf = np.argmax(tfidf_matrix)\n",
    "    v = most_used_words_tfidf\n",
    "    u = inverted_vocab[int(most_used_words_tfidf)%int(len(tfidf_matrix[0]))]\n",
    "    stop_words.add(u)\n",
    "    l.append(u)\n",
    "    r.append(math.ceil(v/100)) \n",
    "    # To convert the v, the Tfidf number to smaller magnitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(l,r)) # Combine two lists into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_keywords = sorted(d, key=d.get,reverse = True) \n",
    "# sort the dictionary(i.e. keywords) with the values.\n",
    "# print(s_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https / //www.foundersspace.com/…/what-are-bitcoin-and-the-b…/ / blockchain / bitcoin / ? / : / @ / wednesedaymotivation / help / get / hump / day / parisoma / # / 's / fab10 / ! / ycombinator / rankings. / share / spot / shares / 10th / top / anniversary / 2017 / happy / accelerator / angelpad / us / crepes / coworking / fund / stampli / week / pymnts.com / series / announcement / coverage / gets / last / following / great / stanbic / ibtc / challenge / stock / think / solution / weecare / customers / challenge. / improve / online / experience / bit.ly/sicc2018 / info / - / innospring / $ / , / apply / zeevi / personalized / microbiome / nutrition / gut / based / david / //blog.ycombinator.com/david-zeevi-on-personalized-n…/ / s18 / ’ / startups / today / http / launched / demo / 59 / wework / combinator / summer / //ow.ly/gpep30lvmgl / 2 / positive / camp / energy / experiencing / . / wwcamp / together / year / global / let / every / gather / begin / camp. / community / ... / adventures / \n"
     ]
    }
   ],
   "source": [
    "s=\"\"\n",
    "for i in s_keywords:\n",
    "    s+= i+\" / \"\n",
    "    \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the list s, our competitors usually uses positive words for the public while they always put links and hashtag on their post to direct and attract traffic to their website/events. Moreover, they use words like \"today\",\"hump day\", \"2017\" and \"happy\" to interact with audience ocassionally. And more formal interaction with \"announcement\" and \"Apply.\"\n",
    "\n",
    "In addition, they also used buzzwords for startups, like \"global\", \"stock\" \"$\", and \"fund.\" (and more~). They also tagged others with '@'.\n",
    "\n",
    "Later stage we can further analyze the list of keywords. Right now, we are only looking at 100 words. When we have more competitors to search OR when we havve more 'posts' saved in our DB, we can further analyze the general strategy and other competitors. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
